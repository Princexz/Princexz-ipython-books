<!doctype html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="IPython Cookbook, ">


    <!-- FAVICON -->
    <link rel="apple-touch-icon" sizes="57x57" href="/apple-touch-icon-57x57.png">
    <link rel="apple-touch-icon" sizes="114x114" href="/apple-touch-icon-114x114.png">
    <link rel="apple-touch-icon" sizes="72x72" href="/apple-touch-icon-72x72.png">
    <link rel="apple-touch-icon" sizes="144x144" href="/apple-touch-icon-144x144.png">
    <link rel="apple-touch-icon" sizes="60x60" href="/apple-touch-icon-60x60.png">
    <link rel="apple-touch-icon" sizes="120x120" href="/apple-touch-icon-120x120.png">
    <link rel="apple-touch-icon" sizes="76x76" href="/apple-touch-icon-76x76.png">
    <link rel="apple-touch-icon" sizes="152x152" href="/apple-touch-icon-152x152.png">
    <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon-180x180.png">
    <link rel="icon" type="image/png" href="/favicon-192x192.png" sizes="192x192">
    <link rel="icon" type="image/png" href="/favicon-160x160.png" sizes="160x160">
    <link rel="icon" type="image/png" href="/favicon-96x96.png" sizes="96x96">
    <link rel="icon" type="image/png" href="/favicon-16x16.png" sizes="16x16">
    <link rel="icon" type="image/png" href="/favicon-32x32.png" sizes="32x32">
    <meta name="msapplication-TileColor" content="#da532c">
    <meta name="msapplication-TileImage" content="/mstile-144x144.png">


        <link rel="alternate"  href="https://ipython-books.github.io/feeds/all.atom.xml" type="application/atom+xml" title="IPython Cookbook Full Atom Feed"/>

        <title>IPython Cookbook - 8.6. Using a random forest to select important features for regression</title>

    <link href="//cdnjs.cloudflare.com/ajax/libs/font-awesome/4.2.0/css/font-awesome.min.css" rel="stylesheet">
    <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/pure/0.3.0/pure-min.css">
    <!--[if lte IE 8]>
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/pure/0.5.0/pure-min.css">
    <![endif]-->
    <!--[if gt IE 8]><!-->
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/pure/0.5.0/pure-min.css">
    <!--<![endif]-->
    <link rel="stylesheet" href="https://ipython-books.github.io/theme/css/styles.css">
    <link rel="stylesheet" href="https://ipython-books.github.io/theme/css/pygments.css">
    <!-- <link href='https://fonts.googleapis.com/css?family=Lato:300,400,700' rel='stylesheet' type='text/css'> -->
    <link href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:300,500" rel="stylesheet" type="text/css">
    <link href='https://fonts.googleapis.com/css?family=Ubuntu+Mono' rel='stylesheet' type='text/css'>
    

    <script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>
</head>

<body>


    <header id="header" class="pure-g">
        <div class="pure-u-1 pure-u-md-3-4">
             <div id="menu">
                 <div class="pure-menu pure-menu-open pure-menu-horizontal">
<ul>
        <li><a href="/">home</a></li>
        <li><a href="https://github.com/ipython-books/cookbook-2nd-code">Jupyter notebooks</a></li>
        <li><a href="https://github.com/ipython-books/minibook-2nd-code">minibook</a></li>
        <li><a href="https://cyrille.rossant.net">author</a></li>
</ul>                </div>
            </div>
        </div>

        <div class="pure-u-1 pure-u-md-1-4">
            <div id="social">
                <div class="pure-menu pure-menu-open pure-menu-horizontal">
<ul>
        <li><a href="https://twitter.com/cyrillerossant"><i class="fa fa-twitter"></i></a></li>
        <li><a href="https://github.com/ipython-books/cookbook-2nd"><i class="fa fa-github"></i></a></li>
</ul>                </div>
            </div>
        </div>
    </header>
       

    
    <div id="layout" class="pure-g">
        <section id="content" class="pure-u-1 pure-u-md-4-4">
            <div class="l-box">

    <header id="page-header">
        <h1>8.6. Using a random forest to select important features for regression</h1>
    </header>

    <section id="page">
        <p><a href="/"><img src="https://raw.githubusercontent.com/ipython-books/cookbook-2nd/master/cover-cookbook-2nd.png" align="left" alt="IPython Cookbook, Second Edition" height="130" style="margin-right: 20px; margin-bottom: 10px;" /></a> <em>This is one of the 100+ free recipes of the <a href="/">IPython Cookbook, Second Edition</a>, by <a href="http://cyrille.rossant.net">Cyrille Rossant</a>, a guide to numerical computing and data science in the Jupyter Notebook. The ebook and printed book are available for purchase at <a href="https://www.packtpub.com/big-data-and-business-intelligence/ipython-interactive-computing-and-visualization-cookbook-second-e">Packt Publishing</a>.</em></p>
<p>▶&nbsp;&nbsp;<em><a href="https://github.com/ipython-books/cookbook-2nd">Text on GitHub</a> with a <a href="https://creativecommons.org/licenses/by-nc-nd/3.0/us/legalcode">CC-BY-NC-ND license</a></em><br />
▶&nbsp;&nbsp;<em><a href="https://github.com/ipython-books/cookbook-2nd-code">Code on GitHub</a> with a <a href="https://opensource.org/licenses/MIT">MIT license</a></em></p>
<p>▶&nbsp;&nbsp;<a href="https://ipython-books.github.io/chapter-8-machine-learning/"><strong><em>Go to</em></strong> <em>Chapter 8 : Machine Learning</em></a><br />
▶&nbsp;&nbsp;<a href="https://github.com/ipython-books/cookbook-2nd-code/blob/master/chapter08_ml/06_random_forest.ipynb"><em><strong>Get</strong> the Jupyter notebook</em></a>  </p>
<p><strong>Decision trees</strong> are frequently used to represent workflows or algorithms. They also form a method for nonparametric supervised learning. A tree mapping observations to target values is learned on a training set and gives the outcomes of new observations.</p>
<p><strong>Random forests</strong> are ensembles of decision trees. Multiple decision trees are trained and aggregated to form a model that is more performant than any of the individual trees. This general idea is the purpose of <strong>ensemble learning</strong>.</p>
<p>There are many types of ensemble methods. Random forests are an instance of <strong>bootstrap aggregating</strong>, also called <strong>bagging</strong>, where models are trained on randomly drawn subsets of the training set.</p>
<p>Random forests yield information about the importance of each feature for the classification or regression task. In this recipe, we will find the most influential features of Boston house prices using a classic dataset that contains a range of diverse indicators about the houses' neighborhood.</p>
<h2>How to do it...</h2>
<p><strong>1.&nbsp;</strong> We import the packages:</p>
<div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">sklearn</span> <span class="kn">as</span> <span class="nn">sk</span>
<span class="kn">import</span> <span class="nn">sklearn.datasets</span> <span class="kn">as</span> <span class="nn">skd</span>
<span class="kn">import</span> <span class="nn">sklearn.ensemble</span> <span class="kn">as</span> <span class="nn">ske</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">plt</span>
<span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>
</pre></div>


<p><strong>2.&nbsp;</strong> We load the Boston dataset:</p>
<div class="highlight"><pre><span></span><span class="n">data</span> <span class="o">=</span> <span class="n">skd</span><span class="o">.</span><span class="n">load_boston</span><span class="p">()</span>
</pre></div>


<p>The details of this dataset can be found in <code>data['DESCR']</code>. Here is the description of some features:</p>
<ul>
<li><em>CRIM</em>: Per capita crime rate by town</li>
<li><em>NOX</em>: Nitric oxide concentration (parts per 10 million)</li>
<li><em>RM</em>: Average number of rooms per dwelling</li>
<li><em>AGE</em>: Proportion of owner-occupied units built prior to 1940</li>
<li><em>DIS</em>: Weighted distances to five Boston employment centres</li>
<li><em>PTRATIO</em>: Pupil-teacher ratio by town</li>
<li><em>LSTAT</em>: Percentage of lower status of the population</li>
<li><em>MEDV</em>: Median value of owner-occupied homes in $1000s</li>
</ul>
<p>The target value is <code>MEDV</code>.</p>
<p><strong>3.&nbsp;</strong> We create a <code>RandomForestRegressor</code> model:</p>
<div class="highlight"><pre><span></span><span class="n">reg</span> <span class="o">=</span> <span class="n">ske</span><span class="o">.</span><span class="n">RandomForestRegressor</span><span class="p">()</span>
</pre></div>


<p><strong>4.&nbsp;</strong> We get the samples and the target values from this dataset:</p>
<div class="highlight"><pre><span></span><span class="n">X</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s1">&#39;data&#39;</span><span class="p">]</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s1">&#39;target&#39;</span><span class="p">]</span>
</pre></div>


<p><strong>5.&nbsp;</strong> Let's fit the model:</p>
<div class="highlight"><pre><span></span><span class="n">reg</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</pre></div>


<p><strong>6.&nbsp;</strong> The importance of our features can be found in <code>reg.feature_importances_</code>. We sort them by decreasing order of importance:</p>
<div class="highlight"><pre><span></span><span class="n">fet_ind</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">reg</span><span class="o">.</span><span class="n">feature_importances_</span><span class="p">)[::</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
<span class="n">fet_imp</span> <span class="o">=</span> <span class="n">reg</span><span class="o">.</span><span class="n">feature_importances_</span><span class="p">[</span><span class="n">fet_ind</span><span class="p">]</span>
</pre></div>


<p><strong>7.&nbsp;</strong> Finally, we plot a histogram of the features' importance by creating a pandas <code>Series</code>:</p>
<div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="n">labels</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s1">&#39;feature_names&#39;</span><span class="p">][</span><span class="n">fet_ind</span><span class="p">]</span>
<span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">fet_imp</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">labels</span><span class="p">)</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="s1">&#39;bar&#39;</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Features importance&#39;</span><span class="p">)</span>
</pre></div>


<p><img alt="&lt;matplotlib.figure.Figure at 0x6fb5d68&gt;" src="https://ipython-books.github.io/pages/chapter08_ml/06_random_forest_files/06_random_forest_22_0.png" /></p>
<p><strong>8.&nbsp;</strong> We find that <em>RM</em> (number of rooms per dwelling) and <em>LSTAT</em> (proportion of lower status of the population) are the most important features determining the price of a house. As an illustration, here is a scatter plot of the price as a function of <em>LSTAT</em>:</p>
<div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">y</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;LSTAT indicator&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Value of houses (k$)&#39;</span><span class="p">)</span>
</pre></div>


<p><img alt="&lt;matplotlib.figure.Figure at 0x7138da0&gt;" src="https://ipython-books.github.io/pages/chapter08_ml/06_random_forest_files/06_random_forest_24_0.png" /></p>
<p><strong>9.&nbsp;</strong> Optionally, we can display a graphic representation of the trees, using the <strong>graphviz</strong> package (available at <a href="http://www.graphviz.org">http://www.graphviz.org</a>):</p>
<div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">tree</span>
<span class="n">tree</span><span class="o">.</span><span class="n">export_graphviz</span><span class="p">(</span><span class="n">reg</span><span class="o">.</span><span class="n">estimators_</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
                     <span class="s1">&#39;tree.dot&#39;</span><span class="p">)</span>
</pre></div>


<p>This command exports the first estimator of the random forest into a <code>.dot</code> file. We can convert this file into an image with the <code>dot</code> command-line executable (available in the graphviz package). The following image shows a small part of the image, which is otherwise too large to display:</p>
<p><img alt="Tree" src="https://ipython-books.github.io/pages/chapter08_ml/06_random_forest_files/tree_zoomout.png" /></p>
<p>The following image shows a close-up of the tree:</p>
<p><img alt="Zoom-out tree" src="https://ipython-books.github.io/pages/chapter08_ml/06_random_forest_files/tree_closeup.png" /></p>
<p>The intermediary nodes contain decisions of the form <code>feature &lt;= value</code>. Every input point starts from the roof and ends up in a leaf node, depending on which conditions are satisfied. The leaf node's value gives the estimated target value for the input point. When using a random forest, an average of the values across trees is computed.</p>
<h2>How it works...</h2>
<p>Several algorithms can be used to train a decision tree. scikit-learn uses the <strong>CART</strong>, or <strong>Classification and Regression Trees algorithm</strong>. This algorithm constructs binary trees using the feature and threshold that yield the largest information gain at each node. Terminal nodes give the outcomes of input values.</p>
<p>Decision trees are simple to understand. They can also be visualized with <strong>pydot</strong>, a Python package for drawing graphs and trees. This is useful when we want to understand what a tree has learned exactly (<strong>white box model</strong>); the conditions that apply on the observations at each node can be expressed easily with Boolean logic.</p>
<p>However, decision trees may suffer from overfitting, notably when they are too deep, and they might be unstable. Additionally, global convergence toward an optimal model is not guaranteed, particularly when greedy algorithms are used for training. These problems can be mitigated by using ensembles of decision trees, notably random forests.</p>
<p>In a random forest, multiple decision trees are trained on bootstrap samples of the training dataset (randomly sampled with replacement). Predictions are made with the averages of individual trees' predictions (bootstrap aggregating or bagging). Additionally, random subsets of the features are chosen at each node (<strong>random subspace method</strong>). These methods lead to an overall better model than the individual trees.</p>
<h2>There's more...</h2>
<p>Here are a few references:</p>
<ul>
<li>Ensemble learning in scikit-learn's documentation, available at <a href="http://scikit-learn.org/stable/modules/ensemble.html">http://scikit-learn.org/stable/modules/ensemble.html</a></li>
<li>API reference of RandomForestRegressor available at <a href="http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html">http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html</a></li>
<li>Random forests on Wikipedia, available at <a href="https://en.wikipedia.org/wiki/Random_forest">https://en.wikipedia.org/wiki/Random_forest</a></li>
<li>Decision tree learning on Wikipedia, available at <a href="https://en.wikipedia.org/wiki/Decision_tree_learning">https://en.wikipedia.org/wiki/Decision_tree_learning</a></li>
<li>Bootstrap aggregating on Wikipedia, available at <a href="https://en.wikipedia.org/wiki/Bootstrap_aggregating">https://en.wikipedia.org/wiki/Bootstrap_aggregating</a></li>
<li>Random subspace method on Wikipedia, available at <a href="https://en.wikipedia.org/wiki/Random_subspace_method">https://en.wikipedia.org/wiki/Random_subspace_method</a></li>
<li>Ensemble learning on Wikipedia, available at <a href="https://en.wikipedia.org/wiki/Ensemble_learning">https://en.wikipedia.org/wiki/Ensemble_learning</a></li>
</ul>
<h2>See also</h2>
<ul>
<li>Using support vector machines for classification tasks</li>
</ul>
    </section>

            </div>
        </section>

        <footer id="footer" class="pure-u-1 pure-u-md-4-4">
            <div class="l-box">
                <div>
                    <p>&copy; <a href="https://cyrille.rossant.net">Cyrille Rossant</a> &ndash;
                        Built with <a href="https://github.com/PurePelicanTheme/pure-single">Pure Theme</a>
                        for <a href="https://blog.getpelican.com/">Pelican</a>
                    </p>
                </div>
            </div>
        </footer>
        
    </div>
    
<!-- Start of StatCounter Code for Default Guide -->
<script type="text/javascript">
var sc_project=9752080; 
var sc_invisible=1; 
var sc_security="c177b501"; 
var scJsHost = (("https:" == document.location.protocol) ?
"https://secure." : "http://www.");
</script>
<script type="text/javascript"
src="https://www.statcounter.com/counter/counter.js"
async></script>
<noscript><div class="statcounter"><a title="Web Analytics"
href="https://statcounter.com/" target="_blank"><img
class="statcounter"
src="//c.statcounter.com/9752080/0/c177b501/1/" alt="Web
Analytics"></a></div></noscript>
<!-- End of StatCounter Code for Default Guide -->
</body>
</html>