<!doctype html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="IPython Cookbook, ">


    <!-- FAVICON -->
    <link rel="apple-touch-icon" sizes="57x57" href="/apple-touch-icon-57x57.png">
    <link rel="apple-touch-icon" sizes="114x114" href="/apple-touch-icon-114x114.png">
    <link rel="apple-touch-icon" sizes="72x72" href="/apple-touch-icon-72x72.png">
    <link rel="apple-touch-icon" sizes="144x144" href="/apple-touch-icon-144x144.png">
    <link rel="apple-touch-icon" sizes="60x60" href="/apple-touch-icon-60x60.png">
    <link rel="apple-touch-icon" sizes="120x120" href="/apple-touch-icon-120x120.png">
    <link rel="apple-touch-icon" sizes="76x76" href="/apple-touch-icon-76x76.png">
    <link rel="apple-touch-icon" sizes="152x152" href="/apple-touch-icon-152x152.png">
    <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon-180x180.png">
    <link rel="icon" type="image/png" href="/favicon-192x192.png" sizes="192x192">
    <link rel="icon" type="image/png" href="/favicon-160x160.png" sizes="160x160">
    <link rel="icon" type="image/png" href="/favicon-96x96.png" sizes="96x96">
    <link rel="icon" type="image/png" href="/favicon-16x16.png" sizes="16x16">
    <link rel="icon" type="image/png" href="/favicon-32x32.png" sizes="32x32">
    <meta name="msapplication-TileColor" content="#da532c">
    <meta name="msapplication-TileImage" content="/mstile-144x144.png">


        <link rel="alternate"  href="https://ipython-books.github.io/feeds/all.atom.xml" type="application/atom+xml" title="IPython Cookbook Full Atom Feed"/>

        <title>IPython Cookbook - 5.11. Performing out-of-core computations on large arrays with Dask</title>

    <link href="//cdnjs.cloudflare.com/ajax/libs/font-awesome/4.2.0/css/font-awesome.min.css" rel="stylesheet">
    <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/pure/0.3.0/pure-min.css">
    <!--[if lte IE 8]>
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/pure/0.5.0/pure-min.css">
    <![endif]-->
    <!--[if gt IE 8]><!-->
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/pure/0.5.0/pure-min.css">
    <!--<![endif]-->
    <link rel="stylesheet" href="https://ipython-books.github.io/theme/css/styles.css">
    <link rel="stylesheet" href="https://ipython-books.github.io/theme/css/pygments.css">
    <!-- <link href='https://fonts.googleapis.com/css?family=Lato:300,400,700' rel='stylesheet' type='text/css'> -->
    <link href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:300,500" rel="stylesheet" type="text/css">
    <link href='https://fonts.googleapis.com/css?family=Ubuntu+Mono' rel='stylesheet' type='text/css'>
    

    <script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>
</head>

<body>


    <header id="header" class="pure-g">
        <div class="pure-u-1 pure-u-md-3-4">
             <div id="menu">
                 <div class="pure-menu pure-menu-open pure-menu-horizontal">
<ul>
        <li><a href="/">home</a></li>
        <li><a href="https://github.com/ipython-books/cookbook-2nd-code">Jupyter notebooks</a></li>
        <li><a href="https://github.com/ipython-books/minibook-2nd-code">minibook</a></li>
        <li><a href="https://cyrille.rossant.net">author</a></li>
</ul>                </div>
            </div>
        </div>

        <div class="pure-u-1 pure-u-md-1-4">
            <div id="social">
                <div class="pure-menu pure-menu-open pure-menu-horizontal">
<ul>
        <li><a href="https://twitter.com/cyrillerossant"><i class="fa fa-twitter"></i></a></li>
        <li><a href="https://github.com/ipython-books/cookbook-2nd"><i class="fa fa-github"></i></a></li>
</ul>                </div>
            </div>
        </div>
    </header>
       

    
    <div id="layout" class="pure-g">
        <section id="content" class="pure-u-1 pure-u-md-4-4">
            <div class="l-box">

    <header id="page-header">
        <h1>5.11. Performing out-of-core computations on large arrays with Dask</h1>
    </header>

    <section id="page">
        <p><a href="/"><img src="https://raw.githubusercontent.com/ipython-books/cookbook-2nd/master/cover-cookbook-2nd.png" align="left" alt="IPython Cookbook, Second Edition" height="130" style="margin-right: 20px; margin-bottom: 10px;" /></a> <em>This is one of the 100+ free recipes of the <a href="/">IPython Cookbook, Second Edition</a>, by <a href="http://cyrille.rossant.net">Cyrille Rossant</a>, a guide to numerical computing and data science in the Jupyter Notebook. The ebook and printed book are available for purchase at <a href="https://www.packtpub.com/big-data-and-business-intelligence/ipython-interactive-computing-and-visualization-cookbook-second-e">Packt Publishing</a>.</em></p>
<p>▶&nbsp;&nbsp;<em><a href="https://github.com/ipython-books/cookbook-2nd">Text on GitHub</a> with a <a href="https://creativecommons.org/licenses/by-nc-nd/3.0/us/legalcode">CC-BY-NC-ND license</a></em><br />
▶&nbsp;&nbsp;<em><a href="https://github.com/ipython-books/cookbook-2nd-code">Code on GitHub</a> with a <a href="https://opensource.org/licenses/MIT">MIT license</a></em></p>
<p>▶&nbsp;&nbsp;<a href="https://ipython-books.github.io/chapter-5-high-performance-computing/"><strong><em>Go to</em></strong> <em>Chapter 5 : High-Performance Computing</em></a><br />
▶&nbsp;&nbsp;<a href="https://github.com/ipython-books/cookbook-2nd-code/blob/master/chapter05_hpc/11_dask.ipynb"><em><strong>Get</strong> the Jupyter notebook</em></a>  </p>
<p><strong>Dask</strong> is a parallel computing library that offers not only a general framework for distributing complex computations on many nodes, but also a set of convenient high-level APIs to deal with out-of-core computations on large arrays. Dask provides data structures resembling NumPy arrays (<em>dask.array</em>) and pandas DataFrames (<em>dask.dataframe</em>) that efficiently scale to huge datasets. The core idea of Dask is to split a large array into smaller arrays (chunks).</p>
<p>In this recipe, we illustrate the basic principles of <em>dask.array</em>.</p>
<h2>Getting started</h2>
<p>Dask should already be installed in Anaconda, but you can always install it manually with <code>conda install dask</code>. You also need <code>memory_profiler</code>, which you can install with <code>conda install memory_profiler</code>.</p>
<h2>How to do it...</h2>
<p><strong>1.&nbsp;</strong> Let's import the libraries:</p>
<div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">dask.array</span> <span class="kn">as</span> <span class="nn">da</span>
<span class="kn">import</span> <span class="nn">memory_profiler</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="o">%</span><span class="n">load_ext</span> <span class="n">memory_profiler</span>
</pre></div>


<p><strong>2.&nbsp;</strong> We initialize a large 10,000 x 10,000 array with random values using dask. The array is chunked into 100 smaller arrays with size 1000 x 1000:</p>
<div class="highlight"><pre><span></span><span class="n">Y</span> <span class="o">=</span> <span class="n">da</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">10000</span><span class="p">,</span> <span class="mi">10000</span><span class="p">),</span>
                     <span class="n">chunks</span><span class="o">=</span><span class="p">(</span><span class="mi">1000</span><span class="p">,</span> <span class="mi">1000</span><span class="p">))</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="n">Y</span>
</pre></div>


<div class="highlight"><pre><span></span>dask.array&lt;da.random.normal, shape=(10000, 10000),
    dtype=float64, chunksize=(1000, 1000)&gt;
</pre></div>


<div class="highlight"><pre><span></span><span class="n">Y</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">Y</span><span class="o">.</span><span class="n">size</span><span class="p">,</span> <span class="n">Y</span><span class="o">.</span><span class="n">chunks</span>
</pre></div>


<div class="highlight"><pre><span></span>((10000, 10000),
 100000000,
 ((1000, ..., 1000),
  (1000, ..., 1000)))
</pre></div>


<p>Memory is not allocated for this huge array. Values will be computed on-the-fly at the last moment.</p>
<p><strong>3.&nbsp;</strong> Let's say we want to compute the mean of every column:</p>
<div class="highlight"><pre><span></span><span class="n">mu</span> <span class="o">=</span> <span class="n">Y</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">mu</span>
</pre></div>


<div class="highlight"><pre><span></span>dask.array&lt;mean_agg-aggregate, shape=(10000,),
    dtype=float64, chunksize=(1000,)&gt;
</pre></div>


<p>This object <code>mu</code> is still a dask array and nothing has been computed yet.</p>
<p><strong>4.&nbsp;</strong> We need to call the <code>compute()</code> method to actually launch the computation. Here, only part of the array is allocated because dask is smart enough to compute just what is necessary for the computation. Here, the 10 chunks containing the first column of the array are allocated and involved in the computation of <code>mu[0]</code>:</p>
<div class="highlight"><pre><span></span><span class="n">mu</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">compute</span><span class="p">()</span>
</pre></div>


<div class="highlight"><pre><span></span>0.011
</pre></div>


<p><strong>5.&nbsp;</strong> Now, we profile the memory usage and time of the same computation using either NumPy or dask.array:</p>
<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">f_numpy</span><span class="p">():</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">10000</span><span class="p">,</span> <span class="mi">10000</span><span class="p">))</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)[</span><span class="mi">0</span><span class="p">:</span><span class="mi">100</span><span class="p">]</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="o">%%</span><span class="n">memit</span>
<span class="n">f_numpy</span><span class="p">()</span>
</pre></div>


<div class="highlight"><pre><span></span>peak memory: 916.32 MiB, increment: 763.00 MiB
</pre></div>


<div class="highlight"><pre><span></span><span class="o">%%</span><span class="n">time</span>
<span class="n">f_numpy</span><span class="p">()</span>
</pre></div>


<div class="highlight"><pre><span></span>CPU times: user 3.86 s, sys: 664 ms, total: 4.52 s
Wall time: 4.52 s
</pre></div>


<p>NumPy used 763 MB to allocate the entire array, and the entire process (allocation and computation) took more than 4 seconds. NumPy wasted time generating all random values and computing the mean of all columns whereas we only requested the first 100 columns.</p>
<p><strong>6.&nbsp;</strong> Next, we use dask.array to perform the same computation:</p>
<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">f_dask</span><span class="p">():</span>
    <span class="n">Y</span> <span class="o">=</span> <span class="n">da</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">10000</span><span class="p">,</span> <span class="mi">10000</span><span class="p">),</span>
                         <span class="n">chunks</span><span class="o">=</span><span class="p">(</span><span class="mi">1000</span><span class="p">,</span> <span class="mi">1000</span><span class="p">))</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">Y</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)[</span><span class="mi">0</span><span class="p">:</span><span class="mi">100</span><span class="p">]</span><span class="o">.</span><span class="n">compute</span><span class="p">()</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="o">%%</span><span class="n">memit</span>
<span class="n">f_dask</span><span class="p">()</span>
</pre></div>


<div class="highlight"><pre><span></span>peak memory: 221.42 MiB, increment: 67.64 MiB
</pre></div>


<div class="highlight"><pre><span></span><span class="o">%%</span><span class="n">time</span>
<span class="n">f_dask</span><span class="p">()</span>
</pre></div>


<div class="highlight"><pre><span></span>CPU times: user 492 ms, sys: 12 ms, total: 504 ms
Wall time: 105 ms
</pre></div>


<p>This time, dask used only 67 MB and the computation lasted about 100 milliseconds.</p>
<p><strong>7.&nbsp;</strong> We can do even better by changing the shape of the chunks. Instead of using 100 square chunks, we use 100 rectangular chunks containing 100 full columns each. The size of the chunks (10,000 elements) remains the same:</p>
<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">f_dask2</span><span class="p">():</span>
    <span class="n">Y</span> <span class="o">=</span> <span class="n">da</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">10000</span><span class="p">,</span> <span class="mi">10000</span><span class="p">),</span>
                         <span class="n">chunks</span><span class="o">=</span><span class="p">(</span><span class="mi">10000</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">Y</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)[</span><span class="mi">0</span><span class="p">:</span><span class="mi">100</span><span class="p">]</span><span class="o">.</span><span class="n">compute</span><span class="p">()</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="o">%%</span><span class="n">memit</span>
<span class="n">f_dask2</span><span class="p">()</span>
</pre></div>


<div class="highlight"><pre><span></span>peak memory: 145.60 MiB, increment: 6.93 MiB
</pre></div>


<div class="highlight"><pre><span></span><span class="o">%%</span><span class="n">time</span>
<span class="n">f_dask2</span><span class="p">()</span>
</pre></div>


<div class="highlight"><pre><span></span>CPU times: user 48 ms, sys: 8 ms, total: 56 ms
Wall time: 57.4 ms
</pre></div>


<p>This is more efficient when computing per-column quantities, because only a single chunk is involved in the computation of the mean of the first 100 columns, compared to 10 chunks in the previous example. The memory usage is therefore 10 times lower here.</p>
<p><strong>8.&nbsp;</strong> Finally, we illustrate how we can use multiple cores to perform computations on large arrays. We create a client using <em>dask.distributed</em>, a distributed computing library that complements dask:</p>
<div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">dask.distributed</span> <span class="kn">import</span> <span class="n">Client</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="n">client</span> <span class="o">=</span> <span class="n">Client</span><span class="p">()</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="n">client</span>
</pre></div>


<p><strong>9.&nbsp;</strong> The computation represented by the <code>Y.sum()</code> dask array can be launch locally, or using the dask.distributed client:</p>
<div class="highlight"><pre><span></span><span class="n">Y</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">compute</span><span class="p">()</span>
</pre></div>


<div class="highlight"><pre><span></span>4090.221
</pre></div>


<div class="highlight"><pre><span></span><span class="n">future</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">compute</span><span class="p">(</span><span class="n">Y</span><span class="o">.</span><span class="n">sum</span><span class="p">())</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="n">future</span>
</pre></div>


<p><img alt="Future" src="https://ipython-books.github.io/pages/chapter05_hpc/11_dask_files/11_dask_41_0.png" /></p>
<div class="highlight"><pre><span></span><span class="n">future</span><span class="o">.</span><span class="n">result</span><span class="p">()</span>
</pre></div>


<div class="highlight"><pre><span></span>4090.221
</pre></div>


<p>The second method scales to large clusters containing many nodes.</p>
<p><strong>10.&nbsp;</strong> We have seen how dask.array could help us manage larger datasets in less memory. Now, we show how we can manipulate arrays that would never fit in our computer. For example, let's compute the average of a large terabyte array:</p>
<div class="highlight"><pre><span></span><span class="n">huge</span> <span class="o">=</span> <span class="n">da</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span>
    <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">1500000</span><span class="p">,</span> <span class="mi">100000</span><span class="p">),</span> <span class="n">chunks</span><span class="o">=</span><span class="p">(</span><span class="mi">10000</span><span class="p">,</span> <span class="mi">10000</span><span class="p">))</span>
<span class="s2">&quot;Size in memory: </span><span class="si">%.1f</span><span class="s2"> GB&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">huge</span><span class="o">.</span><span class="n">nbytes</span> <span class="o">/</span> <span class="mi">1024</span> <span class="o">**</span> <span class="mi">3</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span>&#39;Size in memory: 1117.6 GB&#39;
</pre></div>


<div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">dask.diagnostics</span> <span class="kn">import</span> <span class="n">ProgressBar</span>
<span class="c1"># WARNING: this will take a very long time computing</span>
<span class="c1"># useless values. This is for pedagogical purposes</span>
<span class="c1"># only.</span>
<span class="k">with</span> <span class="n">ProgressBar</span><span class="p">():</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">huge</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="o">.</span><span class="n">compute</span><span class="p">()</span>
</pre></div>


<div class="highlight"><pre><span></span>[##                   ] | 11% Completed |  1min 44.8s
</pre></div>


<p>The way this task is processed, chunk after chunk, can be seen on this graphic showing CPU and RAM usage as a function of time:</p>
<p><img alt="CPU and RAM usage" src="https://ipython-books.github.io/pages/chapter05_hpc/11_dask_files/cpu_ram.png" /></p>
<h2>There's more...</h2>
<p>The dask.array interface shown here is just one of the many possibilities offered by the low-level, graph-based distributed computing framework implemented in Dask. With <strong>task scheduling</strong>, a large computation is split into many smaller computations that may have complex dependencies represented by a dependency graph. A scheduler implements algorithms to execute these computations in parallel by respecting the dependencies.</p>
<p>Here are a few references:</p>
<ul>
<li>Dask documentation at <a href="https://dask.pydata.org/en/latest/index.html">https://dask.pydata.org/en/latest/index.html</a></li>
<li>Integrate Dask with IPython at <a href="http://distributed.readthedocs.io/en/latest/ipython.html">http://distributed.readthedocs.io/en/latest/ipython.html</a></li>
<li>Dask examples at <a href="https://dask.pydata.org/en/latest/examples-tutorials.html">https://dask.pydata.org/en/latest/examples-tutorials.html</a></li>
<li><em>Parallelizing Scientific Python with Dask</em>, by James Crist, SciPy 2017 video tutorial at <a href="https://www.youtube.com/watch?v=mbfsog3e5DA">https://www.youtube.com/watch?v=mbfsog3e5DA</a></li>
<li>Dask tutorial at <a href="https://github.com/dask/dask-tutorial/">https://github.com/dask/dask-tutorial/</a></li>
</ul>
<h2>See also</h2>
<ul>
<li>Distributing Python code across multiple cores with IPython</li>
<li>Interacting with asynchronous parallel tasks in IPython</li>
</ul>
    </section>

            </div>
        </section>

        <footer id="footer" class="pure-u-1 pure-u-md-4-4">
            <div class="l-box">
                <div>
                    <p>&copy; <a href="https://cyrille.rossant.net">Cyrille Rossant</a> &ndash;
                        Built with <a href="https://github.com/PurePelicanTheme/pure-single">Pure Theme</a>
                        for <a href="https://blog.getpelican.com/">Pelican</a>
                    </p>
                </div>
            </div>
        </footer>
        
    </div>
    
<!-- Start of StatCounter Code for Default Guide -->
<script type="text/javascript">
var sc_project=9752080; 
var sc_invisible=1; 
var sc_security="c177b501"; 
var scJsHost = (("https:" == document.location.protocol) ?
"https://secure." : "http://www.");
</script>
<script type="text/javascript"
src="https://www.statcounter.com/counter/counter.js"
async></script>
<noscript><div class="statcounter"><a title="Web Analytics"
href="https://statcounter.com/" target="_blank"><img
class="statcounter"
src="//c.statcounter.com/9752080/0/c177b501/1/" alt="Web
Analytics"></a></div></noscript>
<!-- End of StatCounter Code for Default Guide -->
</body>
</html>